REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL

of 13 June 2024

laying down harmonised rules on artificial intelligence and amending
Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,
(EU) 2018/858, (EU) 2018/1139, (EU) 2018/1882 and Directives
2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)

(Text with EEA relevance)

THE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION,

Having regard to the Treaty on the Functioning of the European Union,
and in particular Articles 16 and 114 thereof,

Having regard to the proposal from the European Commission,

After transmission of the draft legislative act to the national
parliaments,

Having regard to the opinion of the European Economic and Social
Committee,

Having regard to the opinion of the Committee of the Regions,

Acting in accordance with the ordinary legislative procedure,

Whereas:

(1) The purpose of this Regulation is to improve the functioning of the
internal market by laying down a uniform legal framework in particular
for the development, the placing on the market, the putting into service
and the use of artificial intelligence systems in the Union, in
accordance with Union values, to promote the uptake of human centric and
trustworthy artificial intelligence while ensuring a high level of
protection of health, safety, fundamental rights as enshrined in the
Charter of Fundamental Rights of the European Union, including democracy,
the rule of law and environmental protection, to protect against the
harmful effects of artificial intelligence systems in the Union, and to
support innovation.

(2) Artificial intelligence systems can be easily deployed in a large
variety of sectors of the economy and many parts of society, including
across borders, and can easily circulate throughout the Union. Certain
Member States have already explored the adoption of national rules to
ensure that artificial intelligence is trustworthy and safe and is
developed and used in compliance with fundamental rights obligations.

(3) This Regulation should be applied in accordance with the values of the
Union enshrined as in the Charter, facilitating the protection of natural
persons, undertakings, democracy and rule of law, and environmental
protection, while boosting innovation and employment and making the Union
a leader in the uptake of trustworthy artificial intelligence.

(4) At the same time, depending on the circumstances regarding its specific
application and use, artificial intelligence may generate risks and cause
harm to public interests and fundamental rights that are protected by
Union law. Such harm might be material or immaterial, including physical,
psychological, societal or economic harm.

(5) A Union legal framework laying down harmonised rules on artificial
intelligence is therefore needed to foster the development, use and
uptake of artificial intelligence in the internal market that at the same
time meets a high level of protection of public interests, such as
health and safety and the protection of fundamental rights, as recognised
and protected by Union law, including as regards democracy, the rule of
law and environmental protection.

(6) The notion of AI system should be clearly defined and should be closely
aligned with the work of international organisations working on artificial
intelligence to ensure legal certainty, facilitate international
convergence and wide acceptance, while providing sufficient flexibility
to accommodate the rapid technological developments in this field.

CHAPTER I

General provisions

Article 1

Subject matter

1.   The purpose of this Regulation is to improve the functioning of the
internal market and promote the uptake of human-centric and trustworthy
artificial intelligence, while ensuring a high level of protection of
health, safety, fundamental rights enshrined in the Charter of Fundamental
Rights of the European Union, including democracy, the rule of law and
environmental protection, against the harmful effects of artificial
intelligence systems in the Union and supporting innovation.

2.   This Regulation lays down:

(a) harmonised rules for the placing on the market, the putting into
service, and the use of artificial intelligence systems in the Union;

(b) prohibitions of certain artificial intelligence practices;

(c) specific requirements for high-risk artificial intelligence systems and
obligations for operators of such systems;

(d) harmonised transparency rules for certain AI systems;

(e) harmonised rules for the placing on the market of general-purpose AI
models;

(f) rules on market monitoring, market surveillance, governance and
enforcement;

(g) measures to support innovation, with a particular focus on SMEs,
including start-ups.

Article 2

Scope

1.   This Regulation applies to:

(a) providers placing on the market or putting into service AI systems or
placing on the market general-purpose AI models in the Union, irrespective
of whether those providers are established or located within the Union or
in a third country;

(b) deployers of AI systems that have their place of establishment or are
located within the Union;

(c) providers and deployers of AI systems that have their place of
establishment or are located in a third country, where the output produced
by the AI system is used in the Union.

2.   This Regulation does not apply to areas outside the scope of Union law,
and shall not, in any event, affect the competences of the Member States
concerning national security, regardless of the type of entity entrusted
by the Member States with carrying out tasks in relation to those
competences.

3.   This Regulation does not apply to AI systems where and in so far as
they are placed on the market, put into service, or used with or without
modification exclusively for military, defence or national security
purposes, regardless of the type of entity carrying out those activities.

Article 3

Definitions

For the purposes of this Regulation, the following definitions apply:

(1) 'AI system' means a machine-based system that is designed to operate
with varying levels of autonomy and that may exhibit adaptiveness after
deployment, and that, for explicit or implicit objectives, infers, from
the input it receives, how to generate outputs such as predictions,
content, recommendations, or decisions that can influence physical or
virtual environments;

(2) 'risk' means the combination of the probability of an occurrence of
harm and the severity of that harm;

(3) 'provider' means a natural or legal person, public authority, agency
or other body that develops an AI system or a general-purpose AI model or
that has an AI system or a general-purpose AI model developed and places
it on the market or puts the AI system into service under its own name or
trademark, whether for payment or free of charge;

(4) 'deployer' means a natural or legal person, public authority, agency
or other body using an AI system under its authority except where the AI
system is used in the course of a personal non-professional activity;

(5) 'authorised representative' means a natural or legal person located or
established in the Union who has received and accepted a written mandate
from a provider of an AI system or a general-purpose AI model to,
respectively, carry out and perform on its behalf the obligations and
procedures established by this Regulation;

(6) 'importer' means a natural or legal person located or established in
the Union that places on the market an AI system that bears the name or
trademark of a natural or legal person established in a third country;

(7) 'distributor' means a natural or legal person in the supply chain,
other than the provider or the importer, that makes an AI system available
on the Union market;

(8) 'operator' means a provider, product manufacturer, deployer, authorised
representative, importer or distributor;

(9) 'placing on the market' means the first making available of an AI
system or a general-purpose AI model on the Union market;

(10) 'making available on the market' means the supply of an AI system
or a general-purpose AI model for distribution or use on the Union market
in the course of a commercial activity, whether in return for payment or
free of charge;

(11) 'putting into service' means the supply of an AI system for first
use directly to the deployer or for own use in the Union for its intended
purpose;

(12) 'intended purpose' means the use for which an AI system is intended
by the provider, including the specific context and conditions of use,
as specified in the information supplied by the provider in the
instructions for use, promotional or sales materials and statements, as
well as in the technical documentation;

(13) 'reasonably foreseeable misuse' means the use of an AI system in a
way that is not in accordance with its intended purpose, but which may
result from reasonably foreseeable human behaviour or interaction with
other systems, including other AI systems;

(14) 'safety component' means a component of a product or of a system
which fulfils a safety function for that product or system, or the
failure or malfunctioning of which endangers the health and safety of
persons or property;

(15) 'instructions for use' means the information provided by the provider
to inform the deployer of in particular an AI system's intended purpose
and proper use;

(16) 'recall of an AI system' means any measure aimed at achieving the
return to the provider or taking it out of service or disabling the use
of an AI system made available to deployers;

(17) 'withdrawal of an AI system' means any measure aimed at preventing
an AI system in the supply chain from being made available on the market;

(18) 'performance of an AI system' means the ability of an AI system to
achieve its intended purpose;

(19) 'notifying authority' means the national authority responsible for
setting up and carrying out the necessary procedures for the assessment,
designation and notification of conformity assessment bodies and for their
monitoring;

(20) 'conformity assessment' means the process of demonstrating whether
the requirements set out in Chapter III, Section 2 of this Regulation
relating to a high-risk AI system have been fulfilled;

(21) 'conformity assessment body' means a body that performs third-party
conformity assessment activities, including testing, certification and
inspection;

(22) 'notified body' means a conformity assessment body notified in
accordance with this Regulation and other relevant Union harmonisation
legislation;

(23) 'substantial modification' means a change to an AI system after its
placing on the market or putting into service which is not foreseen or
planned in the initial conformity assessment carried out by the provider
and as a result of which the compliance of the AI system with the
requirements set out in Chapter III, Section 2 of this Regulation is
affected or results in a modification to the intended purpose for which
the AI system has been assessed;

(24) 'CE marking' means a marking by which a provider indicates that an
AI system is in conformity with the requirements set out in Chapter III,
Section 2 of this Regulation and other applicable Union harmonisation
legislation providing for its affixing;

(25) 'post-market monitoring system' means all activities carried out by
providers of AI systems to collect and review experience gained from the
use of AI systems they place on the market or put into service for the
purpose of identifying any need to immediately apply any necessary
corrective or preventive actions;

(26) 'market surveillance authority' means the national authority carrying
out the activities and taking the measures pursuant to Regulation (EU)
2019/1020;

(27) 'harmonised standard' means a European standard as defined in
Article 2(1)(c) of Regulation (EU) No 1025/2012;

(28) 'common specification' means a set of technical specifications as
defined in Article 2, point (4) of Regulation (EU) No 1025/2012,
providing means to comply with certain requirements established under
this Regulation;

(29) 'training data' means data used for training an AI system through
fitting its learnable parameters;

(30) 'validation data' means data used for providing an evaluation of the
trained AI system and for tuning its non-learnable parameters and its
learning process in order, inter alia, to prevent underfitting or
overfitting;

(31) 'testing data' means data used for providing an independent evaluation
of the AI system in order to confirm the expected performance of that
system before its placing on the market or putting into service;

(32) 'input data' means data provided to or directly acquired by an AI
system on the basis of which the system produces an output;

(33) 'biometric data' means personal data resulting from specific technical
processing relating to the physical, physiological or behavioural
characteristics of a natural person, such as facial images or
dactyloscopic data;

(34) 'biometric identification' means the automated recognition of
physical, physiological, behavioural, or psychological human features for
the purpose of establishing the identity of a natural person by comparing
biometric data of that individual to biometric data of individuals stored
in a database;

(35) 'biometric verification' means the automated, one-to-one verification,
including authentication, of the identity of natural persons by comparing
their biometric data to previously provided biometric data;

(36) 'special categories of personal data' means the categories of
personal data referred to in Article 9(1) of Regulation (EU) 2016/679,
Article 10 of Directive (EU) 2016/680 and Article 10(1) of Regulation
(EU) 2018/1725;

(37) 'emotion recognition system' means an AI system for the purpose of
identifying or inferring emotions or intentions of natural persons on the
basis of their biometric data;

(38) 'biometric categorisation system' means an AI system for the purpose
of assigning natural persons to specific categories on the basis of their
biometric data, unless it is ancillary to another commercial service and
strictly necessary for objective technical reasons;

(39) 'remote biometric identification system' means an AI system for the
purpose of identifying natural persons, without their active involvement,
typically at a distance through the comparison of a person's biometric
data with the biometric data contained in a reference database;

(40) 'real-time remote biometric identification system' means a remote
biometric identification system, whereby the capturing of biometric data,
the comparison and the identification all occur without a significant
delay, comprising not only instant identification, but also limited short
delays in order to avoid circumvention;

(41) 'post remote biometric identification system' means a remote biometric
identification system other than a real-time remote biometric identification
system;

(42) 'publicly accessible space' means any publicly or privately owned
physical place accessible to an undetermined number of natural persons,
regardless of whether certain conditions for access may apply, and
regardless of the potential capacity restrictions;

(43) 'law enforcement authority' means any public authority competent for
the prevention, investigation, detection or prosecution of criminal
offences or the execution of criminal penalties, including the safeguarding
against and the prevention of threats to public security;

(44) 'law enforcement' means activities carried out by law enforcement
authorities or on their behalf for the prevention, investigation, detection
or prosecution of criminal offences or the execution of criminal penalties,
including the safeguarding against and the prevention of threats to public
security;

(45) 'AI Office' means the Commission's function of contributing to the
implementation, monitoring and supervision of AI systems and general-purpose
AI models, and AI governance, established by Commission Decision of
24 January 2024;

(46) 'national competent authority' means a notifying authority or a market
surveillance authority;

(47) 'serious incident' means an incident or malfunctioning of an AI system
that directly or indirectly leads to death, serious harm to health,
serious disruption to public services, serious harm to property or the
environment;

(48) 'personal data' means personal data as defined in Article 4, point (1)
of Regulation (EU) 2016/679;

(49) 'non-personal data' means data other than personal data as defined in
Article 4, point (1) of Regulation (EU) 2016/679;

(50) 'profiling' means profiling as defined in Article 4, point (4) of
Regulation (EU) 2016/679;

(51) 'real-world testing plan' means a document that describes the
objectives, methodology, geographical, population and temporal scope,
monitoring, organisation and conduct of testing in real-world conditions;

(52) 'sandbox plan' means a document agreed between the participating
provider and the competent authority describing the objectives, conditions,
timeframe, methodology and requirements for the activities carried out
within the sandbox;

(53) 'AI regulatory sandbox' means a controlled framework set up by a
competent authority which offers providers or prospective providers of AI
systems the possibility to develop, train, validate and test, where
appropriate in real-world conditions, an innovative AI system, pursuant
to a sandbox plan for a limited time under regulatory oversight;

(54) 'AI literacy' means skills, knowledge and understanding that allow
providers, deployers and affected persons, taking into account their
respective rights and obligations in the context of this Regulation, to
make an informed deployment of AI systems, as well as to gain awareness
about the opportunities and risks of AI and possible harm it can cause;

(55) 'testing in real-world conditions' means the temporary testing of an
AI system for its intended purpose in real-world conditions outside a
laboratory or otherwise simulated environment, with a view to gathering
reliable and robust data and to assessing and verifying the conformity of
the AI system with the requirements of this Regulation;

(56) 'child' means a natural person below the age of 18;

(57) 'free and open-source AI model' means an AI model that is made
available under a free and open-source licence that allows for the access,
usage, modification, and distribution of the model, and whose parameters,
including the weights, the information on the model architecture, and the
information on model usage, are made publicly available.

CHAPTER II

Prohibited artificial intelligence practices

Article 5

Prohibited AI practices

1.   The following artificial intelligence practices shall be prohibited:

(a) the placing on the market, the putting into service or the use of an
AI system that deploys subliminal techniques beyond a person's consciousness
or purposefully manipulative or deceptive techniques, with the objective or
the effect of materially distorting the behaviour of a person or a group of
persons by appreciably impairing their ability to make an informed decision,
thereby causing that person to take a decision that that person would not
have otherwise taken, in a manner that causes or is reasonably likely to
cause that person, another person or group of persons significant harm;

(b) the placing on the market, the putting into service or the use of an
AI system that exploits any of the vulnerabilities of a natural person or
a specific group of persons due to their age, disability or a specific
social or economic situation, with the objective or the effect of
materially distorting the behaviour of that person or a person belonging
to that group in a manner that causes or is reasonably likely to cause
that person or another person significant harm;

(c) the placing on the market, the putting into service or the use of AI
systems for the evaluation or classification of natural persons or groups
of persons over a certain period of time based on their social behaviour
or known, inferred or predicted personal or personality characteristics,
with the social score leading to either or both of the following: (i)
detrimental or unfavourable treatment of certain natural persons or groups
of persons in social contexts that are unrelated to the contexts in which
the data was originally generated or collected; (ii) detrimental or
unfavourable treatment of certain natural persons or groups of persons
that is unjustified or disproportionate to their social behaviour or its
gravity;

(d) the placing on the market, the putting into service for this specific
purpose, or the use of an AI system for making risk assessments of natural
persons in order to assess or predict the risk of a natural person
committing a criminal offence, based solely on the profiling of a natural
person or on assessing their personality traits and characteristics. This
prohibition shall not apply to AI systems used to support the human
assessment of the involvement of a person in a criminal activity, which is
already based on objective and verifiable facts directly linked to a
criminal activity;

(e) the placing on the market, the putting into service for this specific
purpose, or the use of AI systems that create or expand facial recognition
databases through the untargeted scraping of facial images from the internet
or CCTV footage;

(f) the placing on the market, the putting into service for this specific
purpose, or the use of AI systems to infer emotions of a natural person in
the areas of workplace and education institutions, except where the use of
the AI system is intended to be put in place or into the market for medical
or safety reasons;

(g) the placing on the market, the putting into service for this specific
purpose, or the use of biometric categorisation systems that categorise
individually natural persons based on their biometric data to deduce or
infer their race, political opinions, trade union membership, religious or
philosophical beliefs, sex life or sexual orientation. This prohibition
does not cover any labelling or filtering of lawfully acquired biometric
datasets, such as images, based on biometric data or categorising of
biometric data in the area of law enforcement;

(h) the use of real-time remote biometric identification systems in
publicly accessible spaces for the purposes of law enforcement, unless and
in so far as such use is strictly necessary for one of the following
objectives: (i) the targeted search for specific victims of abduction,
trafficking in human beings or sexual exploitation of human beings, as well
as the search for missing persons; (ii) the prevention of a specific,
substantial and imminent threat to the life or physical safety of natural
persons or a genuine and present or genuine and foreseeable threat of a
terrorist attack; (iii) the localisation or identification of a person
suspected of having committed a criminal offence, for the purpose of
conducting a criminal investigation or prosecution or executing a criminal
penalty for offences referred to in Annex II and punishable in the Member
State concerned by a custodial sentence or a detention order for a maximum
period of at least four years.

Article 6

Classification rules for high-risk AI systems

1.   Irrespective of whether an AI system is placed on the market or put
into service independently of the products referred to in points (a) and
(b), that AI system shall be considered to be high-risk where both of the
following conditions are fulfilled:

(a) the AI system is intended to be used as a safety component of a product,
or the AI system is itself a product, covered by the Union harmonisation
legislation listed in Annex I;

(b) the product whose safety component pursuant to point (a) is the AI
system, or the AI system itself as a product, is required to undergo a
third-party conformity assessment, with a view to the placing on the
market or the putting into service of that product pursuant to the Union
harmonisation legislation listed in Annex I.

2.   In addition to the high-risk AI systems referred to in paragraph 1,
AI systems referred to in Annex III shall be considered to be high-risk.

3.   By derogation from paragraph 2, an AI system referred to in Annex III
shall not be considered to be high-risk where it does not pose a
significant risk of harm to the health, safety or fundamental rights of
natural persons, including by not materially influencing the outcome of
decision making.

CHAPTER III

High-risk AI systems

Section 1

Classification of AI systems as high-risk

Article 9

Risk management system

1.   A risk management system shall be established, implemented, documented
and maintained in relation to high-risk AI systems.

2.   The risk management system shall be understood as a continuous iterative
process planned and run throughout the entire lifecycle of a high-risk AI
system, requiring regular systematic review and updating. It shall comprise
the following steps:

(a) the identification and analysis of the known and the reasonably
foreseeable risks that the high-risk AI system can pose to health, safety
or fundamental rights when the high-risk AI system is used in accordance
with its intended purpose;

(b) the estimation and evaluation of the risks that may emerge when the
high-risk AI system is used in accordance with its intended purpose and
under conditions of reasonably foreseeable misuse;

(c) the evaluation of other risks possibly arising, based on the analysis
of data gathered from the post-market monitoring system referred to in
Article 72;

(d) the adoption of appropriate and targeted risk management measures
designed to address the risks identified pursuant to point (a).

3.   The risk management measures referred to in paragraph 2, point (d)
shall give due consideration to the effects and possible interactions
resulting from the combined application of the requirements set out in
this Section 2. They shall take into account the generally acknowledged
state of the art, including as reflected in relevant harmonised standards
or common specifications.

4.   The risk management measures referred to in paragraph 2, point (d)
shall be such that the relevant residual risk associated with each hazard,
as well as the overall residual risk of the high-risk AI systems is judged
to be acceptable.

Article 10

Data and data governance

1.   High-risk AI systems which make use of techniques involving the
training of AI models with data shall be developed on the basis of training,
validation and testing data sets that meet the quality criteria referred to
in paragraphs 2 to 5.

2.   Training, validation and testing data sets shall be subject to data
management and governance practices appropriate for the intended purpose
of the high-risk AI system. Those practices shall concern in particular:

(a) the relevant design choices;

(b) data collection processes and the origin of data, and in the case of
personal data, the original purpose of the data collection;

(c) relevant data-preparation processing operations, such as annotation,
labelling, cleaning, updating, enrichment and aggregation;

(d) the formulation of assumptions, in particular with respect to the
information that the data are supposed to measure and represent;

(e) an assessment of the availability, quantity and suitability of the
data sets that are needed;

(f) examination in view of possible biases that are likely to affect the
health and safety of persons, have a negative impact on fundamental rights
or lead to discrimination prohibited under Union law, in particular where
data outputs influence inputs for future operations;

(g) appropriate measures to detect, prevent and mitigate possible biases
identified according to point (f);

(h) the identification of relevant data gaps or shortcomings that prevent
the application of this Regulation, and how those gaps and shortcomings
can be addressed.

3.   Training, validation and testing data sets shall be relevant,
sufficiently representative, and to the best extent possible, free of
errors and complete in view of the intended purpose. They shall have the
appropriate statistical properties, including, where applicable, as regards
the persons or groups of persons in relation to whom the high-risk AI
system is intended to be used.

4.   To the extent that it is strictly necessary for the purposes of
ensuring bias detection and correction in relation to the high-risk AI
systems in accordance with paragraph 2, points (f) and (g) of this
Article, the providers of such systems may exceptionally process special
categories of personal data referred to in Article 9(1) of Regulation (EU)
2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of
Regulation (EU) 2018/1725, subject to appropriate safeguards for the
fundamental rights and freedoms of natural persons.

Article 11

Technical documentation

1.   The technical documentation of a high-risk AI system shall be drawn
up before that system is placed on the market or put into service and
shall be kept up to date.

2.   The technical documentation shall be drawn up in such a way as to
demonstrate that the high-risk AI system complies with the requirements
set out in this Section and to provide national competent authorities and
notified bodies with the necessary information in a clear and
comprehensive form to assess the compliance of the AI system with those
requirements.

Article 12

Record-keeping

1.   High-risk AI systems shall technically allow for the automatic
recording of events (logs) over the lifetime of the system.

2.   In order to ensure a level of traceability of the functioning of a
high-risk AI system that is appropriate to the intended purpose of the
system, logging capabilities shall enable the recording of events
relevant for identifying situations that may result in the high-risk AI
system presenting a risk within the meaning of Article 79(1) or lead to
a substantial modification.

Article 13

Transparency and provision of information to deployers

1.   High-risk AI systems shall be designed and developed in such a way as
to ensure that their operation is sufficiently transparent to enable
deployers to interpret a system's output and use it appropriately. An
appropriate type and degree of transparency shall be ensured with a view
to achieving compliance with the relevant obligations of the provider and
deployer set out in Section 3.

2.   High-risk AI systems shall be accompanied by instructions for use in
an appropriate digital format or otherwise that include concise, complete,
correct and clear information that is relevant, accessible and comprehensible
to deployers.

Article 14

Human oversight

1.   High-risk AI systems shall be designed and developed in such a way,
including with appropriate human-machine interface tools, that they can be
effectively overseen by natural persons during the period in which they
are in use.

2.   Human oversight shall aim to prevent or minimise the risks to health,
safety or fundamental rights that may emerge when a high-risk AI system is
used in accordance with its intended purpose or under conditions of
reasonably foreseeable misuse, in particular where such risks persist
despite the application of other requirements set out in this Section.

3.   The oversight measures shall be commensurate with the risks, level of
autonomy and context of use of the high-risk AI system, and shall be
ensured through either one or both of the following types of measures:

(a) measures identified and built, when technically feasible, into the
high-risk AI system by the provider before it is placed on the market or
put into service;

(b) measures identified by the provider before placing the high-risk AI
system on the market or putting it into service and that are appropriate
to be implemented by the deployer.

Article 15

Accuracy, robustness and cybersecurity

1.   High-risk AI systems shall be designed and developed in such a way
that they achieve an appropriate level of accuracy, robustness, and
cybersecurity, and that they perform consistently in those respects
throughout their lifecycle.

2.   To address technical inaccuracies of AI systems that may lead to
outputs that are not correct, or otherwise do not represent reality,
technical solutions shall be developed and integrated.

3.   The robustness of high-risk AI systems may be achieved through
technical redundancy solutions, which may include backup or fail-safe plans.

4.   High-risk AI systems that continue to learn after being placed on the
market or put into service shall be developed in such a way as to
eliminate or reduce as far as possible the risk of possibly biased outputs
influencing input for future operations and as to ensure that any such
feedback loops are duly addressed with appropriate mitigation measures.

5.   High-risk AI systems shall be resilient against attempts by
unauthorised third parties to alter their use, outputs or performance by
exploiting system vulnerabilities. The technical solutions aiming to
ensure the cybersecurity of high-risk AI systems shall be appropriate to
the relevant circumstances and the risks.

CHAPTER IV

Transparency obligations for providers and deployers of certain AI systems

Article 50

Transparency obligations for providers and deployers of certain AI systems

1.   Providers shall ensure that AI systems intended to interact directly
with natural persons are designed and developed in such a way that the
natural person concerned is informed that they are interacting with an AI
system, unless this is obvious from the point of view of a natural person
who is reasonably well-informed, observant and circumspect, taking into
account the circumstances and the context of use. This obligation shall
not apply to AI systems authorised by law to detect, prevent, investigate
or prosecute criminal offences, subject to appropriate safeguards for the
rights and freedoms of third parties, unless those systems are available
for the public to report a criminal offence.

2.   Providers of AI systems, including general-purpose AI systems,
generating synthetic audio, image, video or text content, shall ensure
that the outputs of the AI system are marked in a machine-readable format
and detectable as artificially generated or manipulated. Providers shall
ensure that their technical solutions are effective, interoperable, robust
and reliable as far as this is technically feasible, taking into account
the specificities and limitations of various types of content, the costs
of implementation and the generally acknowledged state of the art, as may
be reflected in relevant technical standards.

3.   Deployers of an emotion recognition system or a biometric
categorisation system shall inform the natural persons exposed thereto of
the operation of the system, and shall process the personal data in
accordance with Regulation (EU) 2016/679, Directive (EU) 2016/680 and
Regulation (EU) 2018/1725, as applicable. This obligation shall not
apply to AI systems used for biometric categorisation and emotion
recognition, which are permitted by law to detect, prevent or investigate
criminal offences, subject to appropriate safeguards for the rights and
freedoms of third parties.

4.   Deployers of an AI system that generates or manipulates image, audio
or video content constituting a deep fake, shall disclose that the content
has been artificially generated or manipulated. This obligation shall not
apply where the use is authorised by law to detect, prevent, investigate
or prosecute criminal offences.

5.   The information referred to in paragraphs 1 to 4 shall be provided
to the natural persons concerned in a clear and distinguishable manner at
the latest at the time of the first interaction or exposure. The
information shall conform to the applicable accessibility requirements.

CHAPTER X

Penalties

Article 71

Fines

1.   In accordance with the terms and conditions laid down in this
Regulation, Member States shall lay down the rules on penalties, including
administrative fines, applicable to infringements of this Regulation by
operators, and shall take all measures necessary to ensure that they are
properly and effectively implemented.

2.   The following infringements shall be subject to administrative fines
of up to 35 000 000 EUR or, if the offender is an undertaking, up to 7 %
of its total worldwide annual turnover for the preceding financial year,
whichever is higher:

(a) non-compliance with the prohibition of the artificial intelligence
practices referred to in Article 5;

(b) non-compliance of the AI system with the requirements laid down in
Article 10.

3.   Non-compliance of the AI system with any requirements or obligations
under this Regulation, other than those laid down in Articles 5 and 10,
shall be subject to administrative fines of up to 15 000 000 EUR or, if
the offender is an undertaking, up to 3 % of its total worldwide annual
turnover for the preceding financial year, whichever is higher.

4.   The supply of incorrect, incomplete or misleading information to
notified bodies or national competent authorities in reply to a request
shall be subject to administrative fines of up to 7 500 000 EUR or, if
the offender is an undertaking, up to 1 % of its total worldwide annual
turnover for the preceding financial year, whichever is higher.

5.   In the case of SMEs, including start-ups, each fine referred to in
this Article shall be up to the percentages or amount referred to in
paragraphs 2, 3 and 4, whichever is lower.

Article 72

Administrative fines on Union institutions, bodies, offices and agencies

1.   The European Data Protection Supervisor may impose administrative
fines on Union institutions, bodies, offices and agencies falling within
the scope of this Regulation. When deciding whether to impose an
administrative fine and deciding on the amount of the administrative fine
in each individual case, all relevant circumstances of the specific
situation shall be taken into account and due regard shall be given to the
following:

(a) the nature, gravity and duration of the infringement and of its
consequences, taking into account the purpose of the AI system concerned,
as well as, where appropriate, the number of affected persons and the
level of damage suffered by them;

(b) whether administrative fines have been already applied by other market
surveillance authorities to the same operator for the same infringement;

(c) whether administrative fines have already been applied by the European
Data Protection Supervisor against the same operator;

(d) the size, annual turnover and market share of the operator committing
the infringement;

(e) any other aggravating or mitigating factor applicable to the
circumstances of the case, such as financial benefits gained, or losses
avoided, directly or indirectly, from the infringement;

(f) the degree of cooperation with the European Data Protection Supervisor,
in order to remedy the infringement and mitigate the possible adverse
effects of the infringement;

(g) the manner in which the infringement became known to the European Data
Protection Supervisor.

2.   Non-compliance with the prohibition of the artificial intelligence
practices referred to in Article 5 shall be subject to administrative
fines of up to 1 500 000 EUR.

3.   The non-compliance of the AI system with any requirements or
obligations under this Regulation, other than those laid down in Article 5,
shall be subject to administrative fines of up to 750 000 EUR.

4.   Before taking decisions pursuant to this Article, the European Data
Protection Supervisor shall give the Union institution, body, office or
agency which is the subject of the proceedings conducted by the European
Data Protection Supervisor the opportunity of being heard on the matter
regarding the possible infringement.
